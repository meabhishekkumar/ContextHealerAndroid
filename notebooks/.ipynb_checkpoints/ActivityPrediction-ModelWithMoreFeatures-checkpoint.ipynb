{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Feature engineering \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Charts\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Static params\n",
    "DATA_FOLDER = 'Data/'\n",
    "DATA_FILE = 'raw_data_fixed.txt'\n",
    "\n",
    "\n",
    "class ActitrackerLR:\n",
    "    ''' Logistic Regression models\n",
    "        one for each class\n",
    "    '''\n",
    "    @staticmethod\n",
    "    def train_models(X_train, Y_train, model_params):\n",
    "        ''' Train models iteratively \n",
    "            for each class\n",
    "        '''\n",
    "        models = [] \n",
    "        for i in xrange(Y_train.shape[1]):\n",
    "            model = LogisticRegression(**model_params)\n",
    "            y = Y_train[:,i]\n",
    "            model.fit(X_train, y)\n",
    "            models.append(model)\n",
    "        return models\n",
    "    \n",
    "    @staticmethod\n",
    "    def make_predictions(X_test, models, num_classes=6):\n",
    "        ''' Make predictions \n",
    "            for each class \n",
    "        '''\n",
    "        predictions = np.zeros((X_test.shape[0], num_classes))\n",
    "        for i, model in enumerate(models):\n",
    "            p = model.predict_proba(X_test)\n",
    "            predictions[:,i] = p[:,1]\n",
    "        return predictions\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    global actitracker\n",
    "    actitracker = pd.read_csv(\n",
    "        DATA_FOLDER+DATA_FILE ,\n",
    "        sep=',' ,\n",
    "        lineterminator=';' ,\n",
    "        header=None ,\n",
    "    )\n",
    "    actitracker.columns = [\n",
    "        'user' ,\n",
    "        'activity' ,\n",
    "        'timestamp' ,\n",
    "        'x-accel' ,\n",
    "        'y-accel' ,\n",
    "        'z-accel' ,\n",
    "        'NA' ,\n",
    "    ]\n",
    "    del actitracker['NA']\n",
    "\n",
    "\n",
    "def create_sessions():\n",
    "    global actitracker\n",
    "    # re-calculate time in seconds\n",
    "    actitracker['time_seconds'] = actitracker['timestamp']*10e-9\n",
    "\n",
    "    # sort by user and time \n",
    "    actitracker = actitracker.sort_values(by=['user','time_seconds'])\n",
    "\n",
    "    # create sessions\n",
    "    session_length = 100\n",
    "    actitracker['seq'] = xrange(actitracker.shape[0])\n",
    "    actitracker['session'] = actitracker.\\\n",
    "                               groupby(['user','activity'])['seq'].\\\n",
    "                               apply(lambda x: x%session_length == 0).\\\n",
    "                               fillna(0).cumsum()\n",
    "\n",
    "\n",
    "def gather_target_vars():\n",
    "    global label_lookup\n",
    "    # get session_labels \n",
    "    ohe = OneHotEncoder(sparse=False); le = LabelEncoder()\n",
    "    labels = actitracker.groupby(['user','session'])['activity'].apply(lambda x: max(x))\n",
    "    le_labels = le.fit_transform(labels)\n",
    "    ohe_labels = ohe.fit_transform(le_labels.reshape(-1,1))\n",
    "    label_lookup = { k: v for k,v in set((i, v) for i,v in np.vstack((le_labels,labels)).T) }\n",
    "    \n",
    "    # create target variables\n",
    "    Y = pd.DataFrame(ohe_labels,index=labels.index)\n",
    "    return Y,labels\n",
    "\n",
    "\n",
    "get_label = np.vectorize(lambda x: label_lookup[x])\n",
    "\n",
    "\n",
    "def feature_engineering():\n",
    "    # group by user and session\n",
    "    accel_cols = ['x-accel','y-accel','z-accel']\n",
    "    g = actitracker.loc[:,accel_cols+['user','session']].groupby(['user','session'])\n",
    "\n",
    "    # IQR function\n",
    "    def iqr(x):\n",
    "        ''' calculate IQR from array\n",
    "        '''\n",
    "        q75, q25 = np.percentile(x, [75,25])\n",
    "        return q75-q25\n",
    "\n",
    "    # calculate model cols \n",
    "    means = g[accel_cols].apply(lambda x: np.mean(x))\n",
    "    sds = g[accel_cols].apply(lambda x: np.std(x))\n",
    "    median_1 = g[accel_cols[0]].apply(lambda x: np.median(x))\n",
    "    median_2 = g[accel_cols[1]].apply(lambda x: np.median(x))\n",
    "    median_3 = g[accel_cols[2]].apply(lambda x: np.median(x))\n",
    "    iqr_1 = g[accel_cols[0]].apply(lambda x: iqr(x))\n",
    "    iqr_2 = g[accel_cols[1]].apply(lambda x: iqr(x))\n",
    "    iqr_3 = g[accel_cols[2]].apply(lambda x: iqr(x))\n",
    "    mins = g[accel_cols].apply(lambda x: np.min(x))\n",
    "    maxs = g[accel_cols].apply(lambda x: np.max(x))\n",
    "    kurtosis_1 = g[accel_cols[0]].apply(lambda x: sp.stats.kurtosis(x))\n",
    "    kurtosis_2 = g[accel_cols[1]].apply(lambda x: sp.stats.kurtosis(x))\n",
    "    kurtosis_3 = g[accel_cols[2]].apply(lambda x: sp.stats.kurtosis(x))\n",
    "    skew_1 = g[accel_cols[0]].apply(lambda x: sp.stats.skew(x))\n",
    "    skew_2 = g[accel_cols[1]].apply(lambda x: sp.stats.skew(x))\n",
    "    skew_3 = g[accel_cols[2]].apply(lambda x: sp.stats.skew(x))\n",
    "    percentiles = []\n",
    "    for i in range(10,100,10):\n",
    "        for e in range(1,4):\n",
    "            percentiles.append(eval('g[accel_cols['+str(e-1)+']].apply(lambda x: sp.percentile(x,'+str(i)+'))'))\n",
    "\n",
    "    # concat columns\n",
    "    X = pd.concat([means,\n",
    "                    sds,\n",
    "                   median_1,\n",
    "                   median_2,\n",
    "                   median_3,\n",
    "                   iqr_1,\n",
    "                   iqr_2,\n",
    "                   iqr_3,\n",
    "                   mins,\n",
    "                   maxs,\n",
    "                   kurtosis_1,\n",
    "                   kurtosis_2,\n",
    "                   kurtosis_3,\n",
    "                   skew_1,\n",
    "                   skew_2,\n",
    "                   skew_3,\n",
    "                  ]+percentiles\n",
    "                  ,axis=1)\n",
    "\n",
    "    # Scale data\n",
    "    ss = StandardScaler()\n",
    "    X = ss.fit_transform(X)\n",
    "    return X\n",
    "\n",
    "\n",
    "def lr_evaluate_params(c_values):\n",
    "    accuracies = []\n",
    "    log_losses = []\n",
    "    for c in c_values:\n",
    "        params = {'C':c,'max_iter':1000,'tol':1e-8}\n",
    "        models = lrmodel.train_models(X_train, Y_train, params)\n",
    "        predictions = lrmodel.make_predictions(X_test, models, 6)\n",
    "        accuracy = accuracy_score(np.argmax(Y_test, axis=1), np.argmax(predictions,axis=1))\n",
    "        ll = log_loss(Y_test, predictions)\n",
    "        accuracies.append(accuracy)\n",
    "        log_losses.append(ll)\n",
    "    evaluation = pd.DataFrame({'C':c_values,'accuracy':accuracies,'log_loss':log_losses})\n",
    "    print evaluation\n",
    "    return evaluation\n",
    "\n",
    "\n",
    "def lr_param_charts(c_values, accuracies, log_losses):\n",
    "    plt.figure(figsize=(14, 4))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(np.log(c_values), accuracies, 'g')\n",
    "    plt.title(\"Change in Accuracy with Decreasing Regularization\")\n",
    "    plt.xlabel(\"Log of Inv. Regularization Strength (C)\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(np.log(c_values), log_losses, 'b')\n",
    "    plt.title(\"Change in Log-loss with Decreasing Regularization\")\n",
    "    plt.xlabel(\"Log of Inv. Regularization Strength (C)\")\n",
    "    plt.ylabel(\"Log-loss\")\n",
    "\n",
    "\n",
    "def print_accuracy(true_category, pred_category):\n",
    "    print 'Accuracy: {}'.format(accuracy_score(true_category, pred_category ))\n",
    "    print 'Log-loss: {}'.format(log_loss(Y_test, predictions))\n",
    "\n",
    "\n",
    "def analyze_errors(true_category, pred_category, get_label=get_label):\n",
    "    errors = pred_category != true_category\n",
    "    true_labels = get_label(true_category)\n",
    "    base = np.ones(true_labels.shape)\n",
    "    error_base = pd.DataFrame({'errors':errors,\n",
    "                      'labels':true_labels,\n",
    "                      'base':base})\n",
    "    b = error_base.groupby('labels').sum()\n",
    "    error_rates = pd.DataFrame((b['errors']/b['base']).sort_values(ascending=False), columns=['Error Rate'])\n",
    "    error_rates['Total Session Counts'] = b['base']\n",
    "    return error_rates\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    classes = label_lookup.values()\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "load_data()\n",
    "create_sessions()\n",
    "Y, labels = gather_target_vars()\n",
    "X = feature_engineering()\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y.as_matrix(), test_size=0.33, random_state=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Series.unique of user  session\n",
       "1     1           Walking\n",
       "      2           Walking\n",
       "      3           Walking\n",
       "      4           Walking\n",
       "      5           Walking\n",
       "      6           Walking\n",
       "      7           Walking\n",
       "      8           Walking\n",
       "      9           Walking\n",
       "      10          Walking\n",
       "      11          Walking\n",
       "      12          Walking\n",
       "      13          Walking\n",
       "      14          Walking\n",
       "      15          Walking\n",
       "      16          Walking\n",
       "      17          Walking\n",
       "      18          Walking\n",
       "      19          Walking\n",
       "      20          Walking\n",
       "      21          Walking\n",
       "      22          Walking\n",
       "      23          Walking\n",
       "      24          Walking\n",
       "      25          Walking\n",
       "      26          Walking\n",
       "      27          Walking\n",
       "      28          Walking\n",
       "      29          Walking\n",
       "      30          Walking\n",
       "                   ...   \n",
       "36    10954       Sitting\n",
       "      10955       Sitting\n",
       "      10956       Sitting\n",
       "      10957       Sitting\n",
       "      10958       Sitting\n",
       "      10959       Sitting\n",
       "      10960       Sitting\n",
       "      10961       Sitting\n",
       "      10962       Sitting\n",
       "      10963      Standing\n",
       "      10964      Standing\n",
       "      10965      Standing\n",
       "      10966      Standing\n",
       "      10967      Standing\n",
       "      10968      Standing\n",
       "      10969      Standing\n",
       "      10970      Standing\n",
       "      10971      Standing\n",
       "      10972      Standing\n",
       "      10973      Standing\n",
       "      10974      Standing\n",
       "      10975      Standing\n",
       "      10976      Standing\n",
       "      10977      Standing\n",
       "      10978      Standing\n",
       "      10979      Standing\n",
       "      10980      Standing\n",
       "      10981      Standing\n",
       "      10982      Standing\n",
       "      10983      Standing\n",
       "Name: activity, dtype: object>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user</th>\n",
       "      <th>session</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"30\" valign=\"top\">1</th>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"30\" valign=\"top\">36</th>\n",
       "      <th>10954</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10955</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10956</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10957</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10958</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10959</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10960</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10961</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10962</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10963</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10964</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10965</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10966</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10967</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10968</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10969</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10970</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10971</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10972</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10973</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10974</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10975</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10976</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10977</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10978</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10979</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10980</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10981</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10982</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10983</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11018 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0  1  2  3  4  5\n",
       "user session                  \n",
       "1    1        0  0  0  0  0  1\n",
       "     2        0  0  0  0  0  1\n",
       "     3        0  0  0  0  0  1\n",
       "     4        0  0  0  0  0  1\n",
       "     5        0  0  0  0  0  1\n",
       "     6        0  0  0  0  0  1\n",
       "     7        0  0  0  0  0  1\n",
       "     8        0  0  0  0  0  1\n",
       "     9        0  0  0  0  0  1\n",
       "     10       0  0  0  0  0  1\n",
       "     11       0  0  0  0  0  1\n",
       "     12       0  0  0  0  0  1\n",
       "     13       0  0  0  0  0  1\n",
       "     14       0  0  0  0  0  1\n",
       "     15       0  0  0  0  0  1\n",
       "     16       0  0  0  0  0  1\n",
       "     17       0  0  0  0  0  1\n",
       "     18       0  0  0  0  0  1\n",
       "     19       0  0  0  0  0  1\n",
       "     20       0  0  0  0  0  1\n",
       "     21       0  0  0  0  0  1\n",
       "     22       0  0  0  0  0  1\n",
       "     23       0  0  0  0  0  1\n",
       "     24       0  0  0  0  0  1\n",
       "     25       0  0  0  0  0  1\n",
       "     26       0  0  0  0  0  1\n",
       "     27       0  0  0  0  0  1\n",
       "     28       0  0  0  0  0  1\n",
       "     29       0  0  0  0  0  1\n",
       "     30       0  0  0  0  0  1\n",
       "...          .. .. .. .. .. ..\n",
       "36   10954    0  0  1  0  0  0\n",
       "     10955    0  0  1  0  0  0\n",
       "     10956    0  0  1  0  0  0\n",
       "     10957    0  0  1  0  0  0\n",
       "     10958    0  0  1  0  0  0\n",
       "     10959    0  0  1  0  0  0\n",
       "     10960    0  0  1  0  0  0\n",
       "     10961    0  0  1  0  0  0\n",
       "     10962    0  0  1  0  0  0\n",
       "     10963    0  0  0  1  0  0\n",
       "     10964    0  0  0  1  0  0\n",
       "     10965    0  0  0  1  0  0\n",
       "     10966    0  0  0  1  0  0\n",
       "     10967    0  0  0  1  0  0\n",
       "     10968    0  0  0  1  0  0\n",
       "     10969    0  0  0  1  0  0\n",
       "     10970    0  0  0  1  0  0\n",
       "     10971    0  0  0  1  0  0\n",
       "     10972    0  0  0  1  0  0\n",
       "     10973    0  0  0  1  0  0\n",
       "     10974    0  0  0  1  0  0\n",
       "     10975    0  0  0  1  0  0\n",
       "     10976    0  0  0  1  0  0\n",
       "     10977    0  0  0  1  0  0\n",
       "     10978    0  0  0  1  0  0\n",
       "     10979    0  0  0  1  0  0\n",
       "     10980    0  0  0  1  0  0\n",
       "     10981    0  0  0  1  0  0\n",
       "     10982    0  0  0  1  0  0\n",
       "     10983    0  0  0  1  0  0\n",
       "\n",
       "[11018 rows x 6 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.4003247 , -0.3805544 , -0.17791853,  0.13834895, -0.67707807,\n",
       "        0.1862355 ,  1.45689209, -0.45677004, -0.487276  ,  0.14323398,\n",
       "       -0.67898883, -0.37901384,  0.80240762,  0.50062263,  0.08050633,\n",
       "        1.41200524,  0.04279476,  0.8866245 , -0.16974649,  0.25476395,\n",
       "        1.32464603,  0.19608871,  1.25351932,  1.9168293 ,  0.90290446,\n",
       "        0.21110376,  0.13234236,  1.02353982,  0.09213821, -0.03568492,\n",
       "        1.24188142, -0.09534324, -0.15783303,  1.3654427 , -0.27739236,\n",
       "       -0.31567319,  1.45689209, -0.45677004, -0.487276  ,  1.43321083,\n",
       "       -0.67439332, -0.65683225,  1.31815458, -0.72055132, -0.68426837,\n",
       "        1.07560947, -0.72491893, -0.59336155,  1.09926409, -0.69184339,\n",
       "       -0.47611452])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.,  1.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7382, 6)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0010 cost= 1.565006971\n",
      "Epoch: 0020 cost= 1.402346969\n",
      "Epoch: 0030 cost= 1.296683788\n",
      "Epoch: 0040 cost= 1.222968221\n",
      "Epoch: 0050 cost= 1.168118477\n",
      "Epoch: 0060 cost= 1.125343323\n",
      "Epoch: 0070 cost= 1.090854168\n",
      "Epoch: 0080 cost= 1.062356591\n",
      "Epoch: 0090 cost= 1.038365960\n",
      "Epoch: 0100 cost= 1.017859697\n",
      "Epoch: 0110 cost= 1.000106812\n",
      "Epoch: 0120 cost= 0.984566748\n",
      "Epoch: 0130 cost= 0.970830500\n",
      "Epoch: 0140 cost= 0.958582640\n",
      "Epoch: 0150 cost= 0.947575808\n",
      "Epoch: 0160 cost= 0.937612474\n",
      "Epoch: 0170 cost= 0.928534746\n",
      "Epoch: 0180 cost= 0.920215905\n",
      "Epoch: 0190 cost= 0.912549317\n",
      "Epoch: 0200 cost= 0.905449390\n",
      "Epoch: 0210 cost= 0.898843169\n",
      "Epoch: 0220 cost= 0.892671406\n",
      "Epoch: 0230 cost= 0.886881351\n",
      "Epoch: 0240 cost= 0.881430387\n",
      "Epoch: 0250 cost= 0.876282036\n",
      "Epoch: 0260 cost= 0.871404231\n",
      "Epoch: 0270 cost= 0.866769373\n",
      "Epoch: 0280 cost= 0.862355173\n",
      "Epoch: 0290 cost= 0.858139038\n",
      "Epoch: 0300 cost= 0.854103982\n",
      "Epoch: 0310 cost= 0.850233912\n",
      "Epoch: 0320 cost= 0.846514046\n",
      "Epoch: 0330 cost= 0.842933714\n",
      "Epoch: 0340 cost= 0.839480460\n",
      "Epoch: 0350 cost= 0.836145282\n",
      "Epoch: 0360 cost= 0.832919717\n",
      "Epoch: 0370 cost= 0.829795361\n",
      "Epoch: 0380 cost= 0.826764941\n",
      "Epoch: 0390 cost= 0.823822498\n",
      "Epoch: 0400 cost= 0.820962965\n",
      "Epoch: 0410 cost= 0.818180263\n",
      "Epoch: 0420 cost= 0.815469742\n",
      "Epoch: 0430 cost= 0.812827528\n",
      "Epoch: 0440 cost= 0.810249448\n",
      "Epoch: 0450 cost= 0.807732284\n",
      "Epoch: 0460 cost= 0.805272400\n",
      "Epoch: 0470 cost= 0.802868009\n",
      "Epoch: 0480 cost= 0.800513804\n",
      "Epoch: 0490 cost= 0.798209369\n",
      "Epoch: 0500 cost= 0.795951962\n",
      "Epoch: 0510 cost= 0.793738723\n",
      "Epoch: 0520 cost= 0.791568160\n",
      "Epoch: 0530 cost= 0.789438665\n",
      "Epoch: 0540 cost= 0.787348330\n",
      "Epoch: 0550 cost= 0.785295427\n",
      "Epoch: 0560 cost= 0.783278644\n",
      "Epoch: 0570 cost= 0.781296372\n",
      "Epoch: 0580 cost= 0.779346704\n",
      "Epoch: 0590 cost= 0.777430117\n",
      "Epoch: 0600 cost= 0.775543571\n",
      "Epoch: 0610 cost= 0.773687601\n",
      "Epoch: 0620 cost= 0.771860301\n",
      "Epoch: 0630 cost= 0.770060658\n",
      "Epoch: 0640 cost= 0.768288136\n",
      "Epoch: 0650 cost= 0.766542256\n",
      "Epoch: 0660 cost= 0.764820755\n",
      "Epoch: 0670 cost= 0.763124466\n",
      "Epoch: 0680 cost= 0.761451662\n",
      "Epoch: 0690 cost= 0.759802401\n",
      "Epoch: 0700 cost= 0.758175194\n",
      "Epoch: 0710 cost= 0.756570041\n",
      "Epoch: 0720 cost= 0.754986882\n",
      "Epoch: 0730 cost= 0.753423989\n",
      "Epoch: 0740 cost= 0.751881361\n",
      "Epoch: 0750 cost= 0.750357449\n",
      "Epoch: 0760 cost= 0.748855174\n",
      "Epoch: 0770 cost= 0.747370005\n",
      "Epoch: 0780 cost= 0.745903015\n",
      "Epoch: 0790 cost= 0.744454443\n",
      "Epoch: 0800 cost= 0.743023455\n",
      "Epoch: 0810 cost= 0.741610050\n",
      "Epoch: 0820 cost= 0.740212679\n",
      "Epoch: 0830 cost= 0.738831997\n",
      "Epoch: 0840 cost= 0.737467349\n",
      "Epoch: 0850 cost= 0.736117959\n",
      "Epoch: 0860 cost= 0.734784722\n",
      "Epoch: 0870 cost= 0.733466625\n",
      "Epoch: 0880 cost= 0.732162654\n",
      "Epoch: 0890 cost= 0.730873466\n",
      "Epoch: 0900 cost= 0.729598522\n",
      "Epoch: 0910 cost= 0.728337646\n",
      "Epoch: 0920 cost= 0.727091014\n",
      "Epoch: 0930 cost= 0.725856602\n",
      "Epoch: 0940 cost= 0.724635720\n",
      "Epoch: 0950 cost= 0.723428071\n",
      "Epoch: 0960 cost= 0.722233236\n",
      "Epoch: 0970 cost= 0.721050262\n",
      "Epoch: 0980 cost= 0.719880760\n",
      "Epoch: 0990 cost= 0.718722224\n",
      "Epoch: 1000 cost= 0.717575848\n",
      "Optimization Finished!\n",
      "Accuracy: 0.770902\n",
      "0.770902\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import shutil\n",
    "import os.path\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 1000\n",
    "batch_size = 100\n",
    "display_step = 10\n",
    "\n",
    "g = tf.Graph()\n",
    "\n",
    "with g.as_default():\n",
    "    # Create the model\n",
    "    \n",
    "    # tf Graph Input\n",
    "    x = tf.placeholder(tf.float32, [None, 51]) # 3 inputs\n",
    "    y = tf.placeholder(tf.float32, [None, 6]) # 6 classes\n",
    "\n",
    "    # Set model weights\n",
    "    W = tf.Variable(tf.zeros([51, 6]))\n",
    "    b = tf.Variable(tf.zeros([6]))\n",
    "\n",
    "    # Construct model\n",
    "    pred = tf.nn.softmax(tf.matmul(x, W) + b) # Softmax\n",
    "\n",
    "    # Minimize error using cross entropy\n",
    "    cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1))\n",
    "    # Gradient Descent\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "    # Initializing the variables\n",
    "    init = tf.initialize_all_variables()\n",
    "    \n",
    "    \n",
    "    sess = tf.Session()\n",
    "\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        _, c = sess.run([optimizer, cost], feed_dict={x: X_train,y: Y_train})\n",
    "        \n",
    "        # Compute average loss\n",
    "        #avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            print \"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(c)\n",
    "\n",
    "    print \"Optimization Finished!\"\n",
    "\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    # Calculate accuracy for 3000 examples\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    print \"Accuracy:\", accuracy.eval({x: X_test, y: Y_test}, sess)\n",
    "\n",
    "# Store variable\n",
    "_W = W.eval(sess)\n",
    "_b = b.eval(sess)\n",
    "\n",
    "\n",
    "sess.close()\n",
    "\n",
    "#Create new graph for exporting\n",
    "g_2 = tf.Graph()\n",
    "with g_2.as_default():\n",
    "    # Reconstruct graph\n",
    "    x_2 = tf.placeholder(\"float\", [None, 51], name=\"input\")\n",
    "    W_2 = tf.constant(_W, name=\"constant_W\")\n",
    "    b_2 = tf.constant(_b, name=\"constant_b\")\n",
    "    y_2 = tf.nn.softmax(tf.matmul(x_2, W_2) + b_2, name=\"output\")\n",
    "\n",
    "    sess_2 = tf.Session()\n",
    "\n",
    "    init_2 = tf.initialize_all_variables();\n",
    "    sess_2.run(init_2)\n",
    "\n",
    "    \n",
    "    graph_def = g_2.as_graph_def()\n",
    "    \n",
    "    tf.train.write_graph(graph_def, 'Models','activityModelLR.pb', as_text=False)\n",
    "\n",
    "    # Test trained model\n",
    "    y__2 = tf.placeholder(\"float\", [None, 6])\n",
    "    correct_prediction_2 = tf.equal(tf.argmax(y_2, 1), tf.argmax(y__2, 1))\n",
    "    accuracy_2 = tf.reduce_mean(tf.cast(correct_prediction_2, \"float\"))\n",
    "    print(accuracy_2.eval({x_2: X_test, y__2: Y_test}, sess_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0005 cost= 396.260162354\n",
      "Epoch: 0010 cost= 276.588134766\n",
      "Epoch: 0015 cost= 215.371566772\n",
      "Epoch: 0020 cost= 165.162765503\n",
      "Epoch: 0025 cost= 124.462135315\n",
      "Epoch: 0030 cost= 103.507209778\n",
      "Epoch: 0035 cost= 88.064643860\n",
      "Epoch: 0040 cost= 77.016250610\n",
      "Epoch: 0045 cost= 70.202415466\n",
      "Epoch: 0050 cost= 64.329627991\n",
      "Epoch: 0055 cost= 59.109596252\n",
      "Epoch: 0060 cost= 54.552120209\n",
      "Epoch: 0065 cost= 50.528736115\n",
      "Epoch: 0070 cost= 46.903934479\n",
      "Epoch: 0075 cost= 43.589862823\n",
      "Epoch: 0080 cost= 40.719127655\n",
      "Epoch: 0085 cost= 38.154396057\n",
      "Epoch: 0090 cost= 35.920024872\n",
      "Epoch: 0095 cost= 34.145431519\n",
      "Epoch: 0100 cost= 32.502452850\n",
      "Epoch: 0105 cost= 30.978788376\n",
      "Epoch: 0110 cost= 29.559520721\n",
      "Epoch: 0115 cost= 28.282278061\n",
      "Epoch: 0120 cost= 27.093397141\n",
      "Epoch: 0125 cost= 25.995040894\n",
      "Epoch: 0130 cost= 25.040588379\n",
      "Epoch: 0135 cost= 24.146463394\n",
      "Epoch: 0140 cost= 23.320995331\n",
      "Epoch: 0145 cost= 22.508140564\n",
      "Epoch: 0150 cost= 21.761072159\n",
      "Epoch: 0155 cost= 21.017499924\n",
      "Epoch: 0160 cost= 20.350358963\n",
      "Epoch: 0165 cost= 19.653480530\n",
      "Epoch: 0170 cost= 19.019811630\n",
      "Epoch: 0175 cost= 18.444417953\n",
      "Epoch: 0180 cost= 17.871957779\n",
      "Epoch: 0185 cost= 17.324665070\n",
      "Epoch: 0190 cost= 16.814054489\n",
      "Epoch: 0195 cost= 16.314435959\n",
      "Epoch: 0200 cost= 15.870188713\n",
      "Epoch: 0205 cost= 15.409162521\n",
      "Epoch: 0210 cost= 15.002192497\n",
      "Epoch: 0215 cost= 14.575544357\n",
      "Epoch: 0220 cost= 14.167669296\n",
      "Epoch: 0225 cost= 13.787502289\n",
      "Epoch: 0230 cost= 13.443835258\n",
      "Epoch: 0235 cost= 13.131596565\n",
      "Epoch: 0240 cost= 12.779175758\n",
      "Epoch: 0245 cost= 12.445129395\n",
      "Epoch: 0250 cost= 12.118598938\n",
      "Epoch: 0255 cost= 11.820994377\n",
      "Epoch: 0260 cost= 11.513013840\n",
      "Epoch: 0265 cost= 11.247887611\n",
      "Epoch: 0270 cost= 10.980298042\n",
      "Epoch: 0275 cost= 10.726996422\n",
      "Epoch: 0280 cost= 10.494463921\n",
      "Epoch: 0285 cost= 10.236119270\n",
      "Epoch: 0290 cost= 10.000134468\n",
      "Epoch: 0295 cost= 9.774431229\n",
      "Epoch: 0300 cost= 9.535750389\n",
      "Epoch: 0305 cost= 9.334171295\n",
      "Epoch: 0310 cost= 9.129302025\n",
      "Epoch: 0315 cost= 8.961083412\n",
      "Epoch: 0320 cost= 8.714472771\n",
      "Epoch: 0325 cost= 8.508922577\n",
      "Epoch: 0330 cost= 8.324929237\n",
      "Epoch: 0335 cost= 8.115440369\n",
      "Epoch: 0340 cost= 7.937790394\n",
      "Epoch: 0345 cost= 7.768868923\n",
      "Epoch: 0350 cost= 7.588035583\n",
      "Epoch: 0355 cost= 7.447877884\n",
      "Epoch: 0360 cost= 7.282721043\n",
      "Epoch: 0365 cost= 7.139365673\n",
      "Epoch: 0370 cost= 6.963711262\n",
      "Epoch: 0375 cost= 6.817366600\n",
      "Epoch: 0380 cost= 6.673278809\n",
      "Epoch: 0385 cost= 6.530429363\n",
      "Epoch: 0390 cost= 6.390151978\n",
      "Epoch: 0395 cost= 6.255630016\n",
      "Epoch: 0400 cost= 6.123631001\n",
      "Epoch: 0405 cost= 5.996582508\n",
      "Epoch: 0410 cost= 5.872923374\n",
      "Epoch: 0415 cost= 5.751771927\n",
      "Epoch: 0420 cost= 5.633155823\n",
      "Epoch: 0425 cost= 5.519385338\n",
      "Epoch: 0430 cost= 5.408272266\n",
      "Epoch: 0435 cost= 5.298834801\n",
      "Epoch: 0440 cost= 5.192636013\n",
      "Epoch: 0445 cost= 5.088436604\n",
      "Epoch: 0450 cost= 4.986220837\n",
      "Epoch: 0455 cost= 4.886703968\n",
      "Epoch: 0460 cost= 4.789460659\n",
      "Epoch: 0465 cost= 4.695144176\n",
      "Epoch: 0470 cost= 4.606279850\n",
      "Epoch: 0475 cost= 4.519930840\n",
      "Epoch: 0480 cost= 4.435539246\n",
      "Epoch: 0485 cost= 4.367991924\n",
      "Epoch: 0490 cost= 4.294915199\n",
      "Epoch: 0495 cost= 4.239194870\n",
      "Epoch: 0500 cost= 4.161704063\n",
      "Epoch: 0505 cost= 4.062388420\n",
      "Epoch: 0510 cost= 4.004915237\n",
      "Epoch: 0515 cost= 3.932845592\n",
      "Epoch: 0520 cost= 3.854356527\n",
      "Epoch: 0525 cost= 3.820374966\n",
      "Epoch: 0530 cost= 3.733502150\n",
      "Epoch: 0535 cost= 3.646844625\n",
      "Epoch: 0540 cost= 3.602752924\n",
      "Epoch: 0545 cost= 3.517841578\n",
      "Epoch: 0550 cost= 3.457230330\n",
      "Epoch: 0555 cost= 3.446115732\n",
      "Epoch: 0560 cost= 3.357838154\n",
      "Epoch: 0565 cost= 3.287878275\n",
      "Epoch: 0570 cost= 3.226713419\n",
      "Epoch: 0575 cost= 3.211262465\n",
      "Epoch: 0580 cost= 3.104331493\n",
      "Epoch: 0585 cost= 3.069982290\n",
      "Epoch: 0590 cost= 3.001579762\n",
      "Epoch: 0595 cost= 2.953537226\n",
      "Epoch: 0600 cost= 2.934546947\n",
      "Epoch: 0605 cost= 2.837000132\n",
      "Epoch: 0610 cost= 2.775491953\n",
      "Epoch: 0615 cost= 2.756539822\n",
      "Epoch: 0620 cost= 2.684684038\n",
      "Epoch: 0625 cost= 2.646047354\n",
      "Epoch: 0630 cost= 2.572223186\n",
      "Epoch: 0635 cost= 2.596939325\n",
      "Epoch: 0640 cost= 2.507976055\n",
      "Epoch: 0645 cost= 2.432461977\n",
      "Epoch: 0650 cost= 2.489973783\n",
      "Epoch: 0655 cost= 2.404225349\n",
      "Epoch: 0660 cost= 2.342643976\n",
      "Epoch: 0665 cost= 2.294379473\n",
      "Epoch: 0670 cost= 2.253081083\n",
      "Epoch: 0675 cost= 2.182372093\n",
      "Epoch: 0680 cost= 2.170013189\n",
      "Epoch: 0685 cost= 2.128939867\n",
      "Epoch: 0690 cost= 2.085790634\n",
      "Epoch: 0695 cost= 2.037044764\n",
      "Epoch: 0700 cost= 2.041176558\n",
      "Epoch: 0705 cost= 1.949474335\n",
      "Epoch: 0710 cost= 1.991342306\n",
      "Epoch: 0715 cost= 1.880919456\n",
      "Epoch: 0720 cost= 1.879193902\n",
      "Epoch: 0725 cost= 1.802116394\n",
      "Epoch: 0730 cost= 1.800342560\n",
      "Epoch: 0735 cost= 1.776770234\n",
      "Epoch: 0740 cost= 1.726772308\n",
      "Epoch: 0745 cost= 1.747955561\n",
      "Epoch: 0750 cost= 1.687135816\n",
      "Epoch: 0755 cost= 1.676077366\n",
      "Epoch: 0760 cost= 1.599962831\n",
      "Epoch: 0765 cost= 1.566009641\n",
      "Epoch: 0770 cost= 1.582881927\n",
      "Epoch: 0775 cost= 1.561002254\n",
      "Epoch: 0780 cost= 1.507132053\n",
      "Epoch: 0785 cost= 1.486973524\n",
      "Epoch: 0790 cost= 1.470498681\n",
      "Epoch: 0795 cost= 1.425697088\n",
      "Epoch: 0800 cost= 1.463849545\n",
      "Epoch: 0805 cost= 1.447730899\n",
      "Epoch: 0810 cost= 1.357396722\n",
      "Epoch: 0815 cost= 1.331866980\n",
      "Epoch: 0820 cost= 1.273646116\n",
      "Epoch: 0825 cost= 1.250961781\n",
      "Epoch: 0830 cost= 1.250638485\n",
      "Epoch: 0835 cost= 1.216196537\n",
      "Epoch: 0840 cost= 1.223874331\n",
      "Epoch: 0845 cost= 1.183488131\n",
      "Epoch: 0850 cost= 1.182992935\n",
      "Epoch: 0855 cost= 1.153794050\n",
      "Epoch: 0860 cost= 1.153455496\n",
      "Epoch: 0865 cost= 1.126869202\n",
      "Epoch: 0870 cost= 1.134126425\n",
      "Epoch: 0875 cost= 1.093442321\n",
      "Epoch: 0880 cost= 1.042017341\n",
      "Epoch: 0885 cost= 1.044641972\n",
      "Epoch: 0890 cost= 0.989428341\n",
      "Epoch: 0895 cost= 1.012073159\n",
      "Epoch: 0900 cost= 1.001672387\n",
      "Epoch: 0905 cost= 0.961608291\n",
      "Epoch: 0910 cost= 0.968225062\n",
      "Epoch: 0915 cost= 0.935731471\n",
      "Epoch: 0920 cost= 0.924087286\n",
      "Epoch: 0925 cost= 0.896980584\n",
      "Epoch: 0930 cost= 0.870146155\n",
      "Epoch: 0935 cost= 0.846285343\n",
      "Epoch: 0940 cost= 0.854135156\n",
      "Epoch: 0945 cost= 0.836932898\n",
      "Epoch: 0950 cost= 0.821605086\n",
      "Epoch: 0955 cost= 0.796922207\n",
      "Epoch: 0960 cost= 0.775419593\n",
      "Epoch: 0965 cost= 0.789730966\n",
      "Epoch: 0970 cost= 0.758441687\n",
      "Epoch: 0975 cost= 0.787676513\n",
      "Epoch: 0980 cost= 0.723390698\n",
      "Epoch: 0985 cost= 0.736866236\n",
      "Epoch: 0990 cost= 0.741607666\n",
      "Epoch: 0995 cost= 0.727660954\n",
      "Epoch: 1000 cost= 0.738847435\n",
      "Epoch: 1005 cost= 0.711715341\n",
      "Epoch: 1010 cost= 0.779704332\n",
      "Epoch: 1015 cost= 0.671772361\n",
      "Epoch: 1020 cost= 0.642970800\n",
      "Epoch: 1025 cost= 0.636607885\n",
      "Epoch: 1030 cost= 0.656920195\n",
      "Epoch: 1035 cost= 0.660175681\n",
      "Epoch: 1040 cost= 0.621197820\n",
      "Epoch: 1045 cost= 0.605597615\n",
      "Epoch: 1050 cost= 0.587895274\n",
      "Epoch: 1055 cost= 0.647132039\n",
      "Epoch: 1060 cost= 0.658872068\n",
      "Epoch: 1065 cost= 0.703065753\n",
      "Epoch: 1070 cost= 0.703599393\n",
      "Epoch: 1075 cost= 0.659314692\n",
      "Epoch: 1080 cost= 0.596437931\n",
      "Epoch: 1085 cost= 0.627885282\n",
      "Epoch: 1090 cost= 0.604528189\n",
      "Epoch: 1095 cost= 0.563539445\n",
      "Epoch: 1100 cost= 0.649343789\n",
      "Epoch: 1105 cost= 0.590280712\n",
      "Epoch: 1110 cost= 0.509034276\n",
      "Epoch: 1115 cost= 0.546577752\n",
      "Epoch: 1120 cost= 0.464458913\n",
      "Epoch: 1125 cost= 0.446119994\n",
      "Epoch: 1130 cost= 0.439336956\n",
      "Epoch: 1135 cost= 0.491506338\n",
      "Epoch: 1140 cost= 0.545785248\n",
      "Epoch: 1145 cost= 0.507077694\n",
      "Epoch: 1150 cost= 0.464818537\n",
      "Epoch: 1155 cost= 0.526590705\n",
      "Epoch: 1160 cost= 0.424483359\n",
      "Epoch: 1165 cost= 0.384810865\n",
      "Epoch: 1170 cost= 0.430608183\n",
      "Epoch: 1175 cost= 0.386406988\n",
      "Epoch: 1180 cost= 0.366741121\n",
      "Epoch: 1185 cost= 0.330282062\n",
      "Epoch: 1190 cost= 0.330063015\n",
      "Epoch: 1195 cost= 0.387463152\n",
      "Epoch: 1200 cost= 0.381818622\n",
      "Epoch: 1205 cost= 0.340523779\n",
      "Epoch: 1210 cost= 0.315005481\n",
      "Epoch: 1215 cost= 0.316585094\n",
      "Epoch: 1220 cost= 0.354978621\n",
      "Epoch: 1225 cost= 0.361329585\n",
      "Epoch: 1230 cost= 0.286759734\n",
      "Epoch: 1235 cost= 0.308355629\n",
      "Epoch: 1240 cost= 0.286158502\n",
      "Epoch: 1245 cost= 0.327988952\n",
      "Epoch: 1250 cost= 0.269590706\n",
      "Epoch: 1255 cost= 0.267894685\n",
      "Epoch: 1260 cost= 0.267810643\n",
      "Epoch: 1265 cost= 0.327860117\n",
      "Epoch: 1270 cost= 0.308980644\n",
      "Epoch: 1275 cost= 0.274957597\n",
      "Epoch: 1280 cost= 0.239656970\n",
      "Epoch: 1285 cost= 0.256826371\n",
      "Epoch: 1290 cost= 0.264377803\n",
      "Epoch: 1295 cost= 0.260016501\n",
      "Epoch: 1300 cost= 0.255856603\n",
      "Epoch: 1305 cost= 0.230690181\n",
      "Epoch: 1310 cost= 0.222299889\n",
      "Epoch: 1315 cost= 0.198570326\n",
      "Epoch: 1320 cost= 0.236331657\n",
      "Epoch: 1325 cost= 0.184160963\n",
      "Epoch: 1330 cost= 0.231263354\n",
      "Epoch: 1335 cost= 0.251542419\n",
      "Epoch: 1340 cost= 0.197505951\n",
      "Epoch: 1345 cost= 0.191030577\n",
      "Epoch: 1350 cost= 0.227428555\n",
      "Epoch: 1355 cost= 0.175758258\n",
      "Epoch: 1360 cost= 0.199254289\n",
      "Epoch: 1365 cost= 0.191433921\n",
      "Epoch: 1370 cost= 0.211323068\n",
      "Epoch: 1375 cost= 0.176323786\n",
      "Epoch: 1380 cost= 0.188177392\n",
      "Epoch: 1385 cost= 0.176970273\n",
      "Epoch: 1390 cost= 0.198802531\n",
      "Epoch: 1395 cost= 0.182935879\n",
      "Epoch: 1400 cost= 0.227790892\n",
      "Epoch: 1405 cost= 0.265691072\n",
      "Epoch: 1410 cost= 0.188015774\n",
      "Epoch: 1415 cost= 0.145839795\n",
      "Epoch: 1420 cost= 0.142329440\n",
      "Epoch: 1425 cost= 0.149470285\n",
      "Epoch: 1430 cost= 0.152288318\n",
      "Epoch: 1435 cost= 0.150437161\n",
      "Epoch: 1440 cost= 0.114927433\n",
      "Epoch: 1445 cost= 0.138620719\n",
      "Epoch: 1450 cost= 0.155401424\n",
      "Epoch: 1455 cost= 0.190567300\n",
      "Epoch: 1460 cost= 0.173047781\n",
      "Epoch: 1465 cost= 0.121840194\n",
      "Epoch: 1470 cost= 0.133959100\n",
      "Epoch: 1475 cost= 0.129802749\n",
      "Epoch: 1480 cost= 0.179954350\n",
      "Epoch: 1485 cost= 0.177415162\n",
      "Epoch: 1490 cost= 0.155534491\n",
      "Epoch: 1495 cost= 0.162732959\n",
      "Epoch: 1500 cost= 0.111205526\n",
      "Epoch: 1505 cost= 0.126220271\n",
      "Epoch: 1510 cost= 0.114567935\n",
      "Epoch: 1515 cost= 0.155569419\n",
      "Epoch: 1520 cost= 0.214923456\n",
      "Epoch: 1525 cost= 0.115125015\n",
      "Epoch: 1530 cost= 0.121445403\n",
      "Epoch: 1535 cost= 0.146156952\n",
      "Epoch: 1540 cost= 0.147676021\n",
      "Epoch: 1545 cost= 0.178716332\n",
      "Epoch: 1550 cost= 0.139986530\n",
      "Epoch: 1555 cost= 0.266017616\n",
      "Epoch: 1560 cost= 0.213907629\n",
      "Epoch: 1565 cost= 0.103406973\n",
      "Epoch: 1570 cost= 0.251581967\n",
      "Epoch: 1575 cost= 0.242029384\n",
      "Epoch: 1580 cost= 0.219311237\n",
      "Epoch: 1585 cost= 0.164983198\n",
      "Epoch: 1590 cost= 0.103684820\n",
      "Epoch: 1595 cost= 0.144977018\n",
      "Epoch: 1600 cost= 0.185224220\n",
      "Epoch: 1605 cost= 0.077899277\n",
      "Epoch: 1610 cost= 0.105490685\n",
      "Epoch: 1615 cost= 0.111288883\n",
      "Epoch: 1620 cost= 0.118835680\n",
      "Epoch: 1625 cost= 0.120282076\n",
      "Epoch: 1630 cost= 0.150314063\n",
      "Epoch: 1635 cost= 0.074054547\n",
      "Epoch: 1640 cost= 0.150472343\n",
      "Epoch: 1645 cost= 0.101686224\n",
      "Epoch: 1650 cost= 0.071362481\n",
      "Epoch: 1655 cost= 0.104856066\n",
      "Epoch: 1660 cost= 0.054205611\n",
      "Epoch: 1665 cost= 0.057622738\n",
      "Epoch: 1670 cost= 0.053495914\n",
      "Epoch: 1675 cost= 0.065458544\n",
      "Epoch: 1680 cost= 0.104915000\n",
      "Epoch: 1685 cost= 0.097983651\n",
      "Epoch: 1690 cost= 0.177734509\n",
      "Epoch: 1695 cost= 0.247566104\n",
      "Epoch: 1700 cost= 0.252341092\n",
      "Epoch: 1705 cost= 0.183043018\n",
      "Epoch: 1710 cost= 0.171506867\n",
      "Epoch: 1715 cost= 0.196347952\n",
      "Epoch: 1720 cost= 0.161515996\n",
      "Epoch: 1725 cost= 0.136014014\n",
      "Epoch: 1730 cost= 0.108810410\n",
      "Epoch: 1735 cost= 0.104642503\n",
      "Epoch: 1740 cost= 0.079069987\n",
      "Epoch: 1745 cost= 0.125368670\n",
      "Epoch: 1750 cost= 0.081185028\n",
      "Epoch: 1755 cost= 0.077936411\n",
      "Epoch: 1760 cost= 0.101829454\n",
      "Epoch: 1765 cost= 0.071744695\n",
      "Epoch: 1770 cost= 0.139621273\n",
      "Epoch: 1775 cost= 0.075154141\n",
      "Epoch: 1780 cost= 0.062761977\n",
      "Epoch: 1785 cost= 0.082685061\n",
      "Epoch: 1790 cost= 0.062814072\n",
      "Epoch: 1795 cost= 0.052297175\n",
      "Epoch: 1800 cost= 0.044570688\n",
      "Epoch: 1805 cost= 0.041638259\n",
      "Epoch: 1810 cost= 0.089412063\n",
      "Epoch: 1815 cost= 0.145482495\n",
      "Epoch: 1820 cost= 0.170210570\n",
      "Epoch: 1825 cost= 0.108199559\n",
      "Epoch: 1830 cost= 0.054943975\n",
      "Epoch: 1835 cost= 0.111875802\n",
      "Epoch: 1840 cost= 0.184214965\n",
      "Epoch: 1845 cost= 0.143633366\n",
      "Epoch: 1850 cost= 0.085349604\n",
      "Epoch: 1855 cost= 0.065755345\n",
      "Epoch: 1860 cost= 0.047066271\n",
      "Epoch: 1865 cost= 0.071851499\n",
      "Epoch: 1870 cost= 0.077309258\n",
      "Epoch: 1875 cost= 0.168885499\n",
      "Epoch: 1880 cost= 0.255475491\n",
      "Epoch: 1885 cost= 0.223899350\n",
      "Epoch: 1890 cost= 0.181986377\n",
      "Epoch: 1895 cost= 0.211896032\n",
      "Epoch: 1900 cost= 0.221067399\n",
      "Epoch: 1905 cost= 0.182570025\n",
      "Epoch: 1910 cost= 0.069952048\n",
      "Epoch: 1915 cost= 0.165263340\n",
      "Epoch: 1920 cost= 0.240232587\n",
      "Epoch: 1925 cost= 0.076130517\n",
      "Epoch: 1930 cost= 0.129471838\n",
      "Epoch: 1935 cost= 0.316444755\n",
      "Epoch: 1940 cost= 0.206486508\n",
      "Epoch: 1945 cost= 0.123856753\n",
      "Epoch: 1950 cost= 0.201281175\n",
      "Epoch: 1955 cost= 0.087005951\n",
      "Epoch: 1960 cost= 0.060283761\n",
      "Epoch: 1965 cost= 0.075590827\n",
      "Epoch: 1970 cost= 0.092183888\n",
      "Epoch: 1975 cost= 0.070340008\n",
      "Epoch: 1980 cost= 0.137137547\n",
      "Epoch: 1985 cost= 0.049631823\n",
      "Epoch: 1990 cost= 0.016920054\n",
      "Epoch: 1995 cost= 0.022143148\n",
      "Epoch: 2000 cost= 0.058109414\n",
      "Optimization Finished!\n",
      "Accuracy: 0.880913\n",
      "0.880913\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import shutil\n",
    "import os.path\n",
    "\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 2000\n",
    "batch_size = 500\n",
    "display_step = 5\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 200 # 1st layer number of features\n",
    "n_hidden_2 = 200 # 2nd layer number of features\n",
    "n_input = 51 # Number of inputs\n",
    "n_classes = 6 # Number of classes\n",
    "\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    # model inputs\n",
    "    x = tf.placeholder(\"float\", shape=[None, n_input])\n",
    "    y = tf.placeholder(\"float\", shape=[None, n_classes])\n",
    "    \n",
    "    # set model weights\n",
    "    W_h1 = tf.Variable(tf.random_normal([n_input, n_hidden_1]))\n",
    "    W_h2 = tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2]))\n",
    "    W_out = tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n",
    "    \n",
    "    # set model biases\n",
    "    b1 = tf.Variable(tf.random_normal([n_hidden_1]))\n",
    "    b2 = tf.Variable(tf.random_normal([n_hidden_2]))\n",
    "    b_out = tf.Variable(tf.random_normal([n_classes]))\n",
    "    \n",
    "    # Construct Model\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_1 = tf.add(tf.matmul(x, W_h1), b1)\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, W_h2), b2)\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    # Output layer with linear activation\n",
    "    pred = tf.matmul(layer_2, W_out) + b_out\n",
    "    \n",
    "    # Define loss and optimizer\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    \n",
    "    # Initializing the variables\n",
    "    init = tf.initialize_all_variables()\n",
    "    \n",
    "    sess = tf.Session()\n",
    "\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        _, c = sess.run([optimizer, cost], feed_dict={x: X_train,y: Y_train})\n",
    "        \n",
    "        # Compute average loss\n",
    "        #avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            print \"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(c)\n",
    "\n",
    "    print \"Optimization Finished!\"\n",
    "\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    # Calculate accuracy for 3000 examples\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    print \"Accuracy:\", accuracy.eval({x: X_test, y: Y_test}, sess)\n",
    "\n",
    "# Store Variable\n",
    "_W_h1 = W_h1.eval(sess)\n",
    "_W_h2 = W_h2.eval(sess)\n",
    "_W_out =W_out.eval(sess)\n",
    "\n",
    "_b1 = b1.eval(sess)\n",
    "_b2 = b2.eval(sess)\n",
    "_b_out = b_out.eval(sess)\n",
    "\n",
    "sess.close()\n",
    "\n",
    "# create a new graph for exporting\n",
    "g_2 = tf.Graph()\n",
    "with g_2.as_default():\n",
    "    # Reconstruct Graph\n",
    "    # model inputs\n",
    "    x_2 = tf.placeholder(\"float\", shape=[None, n_input], name=\"input\")\n",
    "    \n",
    "    \n",
    "    # set model weights\n",
    "    W_2_h1 = tf.constant(_W_h1, name=\"constant_W_h1\")\n",
    "    W_2_h2 = tf.constant(_W_h2, name=\"constant_W_h2\")\n",
    "    W_2_out = tf.constant(_W_out, name=\"constant_W_out\")\n",
    "    \n",
    "    # set model biases\n",
    "    b_2_1 = tf.constant(_b1, name=\"constant_b1\")\n",
    "    b_2_2 = tf.constant(_b2, name=\"constant_b2\")\n",
    "    b_2_out = tf.constant(_b_out, name=\"constant_b_out\")\n",
    "    \n",
    "    # Construct Model\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_2_1 = tf.add(tf.matmul(x_2, W_2_h1), b_2_1)\n",
    "    layer_2_1 = tf.nn.relu(layer_2_1)\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_2_2 = tf.add(tf.matmul(layer_2_1, W_2_h2), b_2_2)\n",
    "    layer_2_2 = tf.nn.relu(layer_2_2)\n",
    "    \n",
    "    # Output layer with linear activation\n",
    "    y_2 = tf.nn.bias_add(tf.matmul(layer_2_2, W_2_out), b_2_out, name=\"output\")\n",
    "    \n",
    "    #y_2.name = \"output\"\n",
    "    \n",
    "    sess_2 = tf.Session()\n",
    "\n",
    "    init_2 = tf.initialize_all_variables();\n",
    "    sess_2.run(init_2)\n",
    "\n",
    "    \n",
    "    graph_def = g_2.as_graph_def()\n",
    "    \n",
    "    tf.train.write_graph(graph_def, 'Models','activityModelMLP2.pb', as_text=False)\n",
    "\n",
    "    # Test trained model\n",
    "    y__2 = tf.placeholder(\"float\", [None, 6])\n",
    "    correct_prediction_2 = tf.equal(tf.argmax(y_2, 1), tf.argmax(y__2, 1))\n",
    "    accuracy_2 = tf.reduce_mean(tf.cast(correct_prediction_2, \"float\"))\n",
    "    print(accuracy_2.eval({x_2: X_test, y__2: Y_test}, sess_2))\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
